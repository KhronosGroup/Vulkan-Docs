// Copyright (c) 2020-2021 NVIDIA Corporation
//
// SPDX-License-Identifier: CC-BY-4.0

[[cuda-modules]]
== CUDA Modules


[[cuda-modules-creation]]
=== Creating a CUDA Module

[open,refpage='VkCudaModuleNV',desc='Opaque handle to a CUDA module object',type='handles']
--
CUDA modules must: contain some kernel code and must: expose at least one
function entry point.

CUDA modules are represented by sname:VkCudaModuleNV handles:

include::{generated}/api/handles/VkCudaModuleNV.adoc[]
--

[open,refpage='vkCreateCudaModuleNV',desc='Creates a new CUDA module object',type='protos']
--
To create a CUDA module, call:

include::{generated}/api/protos/vkCreateCudaModuleNV.adoc[]

  * pname:device is the logical device that creates the shader module.
  * pname:pCreateInfo is a pointer to a slink:VkCudaModuleCreateInfoNV
    structure.
  * pname:pAllocator controls host memory allocation as described in the
    <<memory-allocation, Memory Allocation>> chapter.
  * pname:pModule is a pointer to a slink:VkCudaModuleNV handle in which the
    resulting CUDA module object is returned.

Once a CUDA module has been created, the application may: create the
function entry point, which must: refer to one function in the module.

include::{generated}/validity/protos/vkCreateCudaModuleNV.adoc[]
--

[open,refpage='VkCudaModuleCreateInfoNV',desc='Structure specifying the parameters to create a CUDA Module',type='structs']
--
The sname:VkCudaModuleCreateInfoNV structure is defined as:

include::{generated}/api/structs/VkCudaModuleCreateInfoNV.adoc[]

  * pname:sType is a elink:VkStructureType value identifying this structure.
  * pname:pNext may: be `NULL` or may: be a pointer to a structure extending
    this structure.
  * pname:dataSize is the length of the pname:pData array.
  * pname:pData is a pointer to CUDA code

.Valid Usage
****
  * [[VUID-VkCudaModuleCreateInfoNV-dataSize-09413]]
    pname:dataSize must: be the total size in bytes of the PTX files or
    binary cache passed to pname:pData
****

include::{generated}/validity/structs/VkCudaModuleCreateInfoNV.adoc[]
--


[[cuda-function-creation]]
=== Creating a CUDA Function Handle

[open,refpage='VkCudaFunctionNV',desc='Opaque handle to a CUDA function object',type='handles']
--
CUDA functions are represented by sname:VkCudaFunctionNV handles.
Handles to `__global__` functions may: then be used to issue a kernel launch
(i.e. dispatch) from a commandbuffer.
See <<cudadispatch, Dispatching Command for CUDA PTX kernel>>.

include::{generated}/api/handles/VkCudaFunctionNV.adoc[]
--

[open,refpage='vkCreateCudaFunctionNV',desc='Creates a new CUDA function object',type='protos']
--
To create a CUDA function, call:

include::{generated}/api/protos/vkCreateCudaFunctionNV.adoc[]

  * pname:device is the logical device that creates the shader module.
  * pname:pCreateInfo is a pointer to a slink:VkCudaFunctionCreateInfoNV
    structure.
  * pname:pAllocator controls host memory allocation as described in the
    <<memory-allocation, Memory Allocation>> chapter.
  * pname:pFunction is a pointer to a slink:VkCudaFunctionNV handle in which
    the resulting CUDA function object is returned.

include::{generated}/validity/protos/vkCreateCudaFunctionNV.adoc[]
--

[open,refpage='VkCudaFunctionCreateInfoNV',desc='Structure specifying the parameters to create a CUDA Function',type='structs']
--
The sname:VkCudaFunctionCreateInfoNV structure is defined as:

include::{generated}/api/structs/VkCudaFunctionCreateInfoNV.adoc[]

  * pname:sType is a elink:VkStructureType value identifying this structure.
  * pname:pNext is `NULL` or a pointer to a structure extending this
    structure.
  * pname:module is the CUDA slink:VkCudaModuleNV module in which the
    function resides.
  * pname:pName is a null-terminated UTF-8 string containing the name of the
    shader entry point for this stage.

include::{generated}/validity/structs/VkCudaFunctionCreateInfoNV.adoc[]
--


[[cuda-function-destruction]]
=== Destroying a CUDA Function

[open,refpage='vkDestroyCudaFunctionNV',desc='Destroy a CUDA function',type='protos']
--
To destroy a CUDA function handle, call:

include::{generated}/api/protos/vkDestroyCudaFunctionNV.adoc[]

  * pname:device is the logical device that destroys the Function.
  * pname:function is the handle of the CUDA function to destroy.
  * pname:pAllocator controls host memory allocation as described in the
    <<memory-allocation, Memory Allocation>> chapter.

include::{generated}/validity/protos/vkDestroyCudaFunctionNV.adoc[]
--


[[cuda-modules-destruction]]
=== Destroying a CUDA Module

[open,refpage='vkDestroyCudaModuleNV',desc='Destroy a CUDA module',type='protos']
--
To destroy a CUDA shader module, call:

include::{generated}/api/protos/vkDestroyCudaModuleNV.adoc[]

  * pname:device is the logical device that destroys the shader module.
  * pname:module is the handle of the CUDA module to destroy.
  * pname:pAllocator controls host memory allocation as described in the
    <<memory-allocation, Memory Allocation>> chapter.

include::{generated}/validity/protos/vkDestroyCudaModuleNV.adoc[]
--


[[cuda-modules-getcache]]
=== Reading back CUDA Module Cache

After uploading the PTX kernel code, the module compiles the code to
generate a binary cache with all the necessary information for the device to
execute it.
It is possible to read back this cache for later use, such as accelerating
the initialization of further executions.

[open,refpage='vkGetCudaModuleCacheNV',desc='Get CUDA module cache',type='protos']
--
To get the CUDA module cache call:

include::{generated}/api/protos/vkGetCudaModuleCacheNV.adoc[]

  * pname:device is the logical device that destroys the Function.
  * pname:module is the CUDA module.
  * pname:pCacheSize is a pointer containing the amount of bytes to be
    copied in pname:pCacheData
  * pname:pCacheData is a pointer to a buffer in which to copy the binary
    cache

If pname:pCacheData is `NULL`, then the size of the binary cache, in bytes,
is returned in pname:pCacheSize.
Otherwise, pname:pCacheSize must: point to a variable set by the user to the
size of the buffer, in bytes, pointed to by pname:pCacheData, and on return
the variable is overwritten with the amount of data actually written to
pname:pCacheData.
If pname:pCacheSize is less than the size of the binary shader code, nothing
is written to pname:pCacheData, and ename:VK_INCOMPLETE will be returned
instead of ename:VK_SUCCESS.

The returned cache may: then be used later for further initialization of the
CUDA module, by sending this cache _instead_ of the PTX code when using
flink:vkCreateCudaModuleNV.

[NOTE]
.Note
====
Using the binary cache instead of the original PTX code should:
significantly speed up initialization of the CUDA module, given that the
whole compilation and validation will not be necessary.

As with slink:VkPipelineCache, the binary cache depends on the specific
implementation.
The application must: assume the cache upload might fail in many
circumstances and thus may: have to get ready for falling back to the
original PTX code if necessary.
Most often, the cache may: succeed if the same device driver and
architecture is used between the cache generation from PTX and the use of
this cache.
In the event of a new driver version or if using a different device
architecture, this cache may: become invalid.
====

include::{generated}/validity/protos/vkGetCudaModuleCacheNV.adoc[]
--


[[cuda-modules-limitations]]
=== Limitations

CUDA and Vulkan do not use the device in the same configuration.
The following limitations must: be taken into account:

  * It is not possible to read or write global parameters from Vulkan.
    The only way to share resources or send values to the PTX kernel is to
    pass them as arguments of the function.
    See <<cudadispatch_sharing_resources, Resources sharing between CUDA
    Kernel and Vulkan>> for more details.
  * No calls to functions external to the module PTX are supported.
  * Vulkan disables some shader/kernel exceptions, which could break CUDA
    kernels relying on exceptions.
  * CUDA kernels submitted to Vulkan are limited to the amount of shared
    memory, which can be queried from the physical capabilities.
    It may be less than what CUDA can offer.
  * CUDA instruction-level preemption (CILP) does not work.
  * CUDA Unified Memory will not work in this extension.
  * CUDA Dynamic parallelism is not supported.
  * ftext:vk*DispatchIndirect is not available.
